# Machine Learning

## 基本知识

### 损失函数(交叉熵,MAE,MSE,Huber)

* 对于分类问题

信息熵：$H(p)=-\sum^n_j p(x_j) \text{log}(p(x_j))$

对于真实概率$P$以及实际概率$Q$，可求得KL散度：$D_{KL}(P \| Q) = \sum_{x \in \mathcal{X}} P(x) \log\left(\frac{P(x)}{Q(x)}\right)=\sum_{x \in \mathcal{X}} (P(x) \text{log}(P(x))-P(x) \text{log}(Q(x)))$

由于$P(x)$固定，一般只用求交叉熵即可：$H(P, Q) = -\sum_{x \in \mathcal{X}} P(x) \log(Q(x))$

实际的$loss$可以对所有样本的交叉熵求平均。`torch.nn.CrossEntropyLoss`

* 对于回归问题

平均绝对误差：只求个绝对值$\text{MAE} = \frac{1}{n}\sum_{i=1}^{n} |y_i - \hat{y}_i|$ 更稳健

均方误差：只求个平方：$\text{MSE} = \frac{1}{n}\sum_{i=1}^{n} (y_i - \hat{y}_i)^2$ 屈服于异常值，但是更稳定(对于小误差而言)

结合稳健性和稳定性，Huber损失：$\text{Huber}(y, \hat{y}) = \begin{cases}
\frac{1}{2}(y - \hat{y})^2, & \text{if } |y - \hat{y}| \leq \delta, \\
\delta(|y - \hat{y}| - \frac{1}{2}\delta), & \text{otherwise}.
\end{cases}$



### 激活函数(sigmoid,ReLU,Softmax,tanh)

* ==sigmoid==    $\sigma(x) = \frac{1}{1 + e^{-x}}$

**二分类**问题的概率预测，结果区间为(0,1)

cons: 梯度消失问题

* ==ReLU== $\text{ReLU}(x) = \max(0, x)$

常用于隐藏层

cons: 死亡ReLU问题（一旦输入负数，ReLU单元会死掉，即永远输出0）

* ==Softmax==     $\text{Softmax}(x_i) = \frac{e^{x_i}}{\sum_{j=1}^{K} e^{x_j}}, \quad \text{for}\ i = 1, 2, ..., K$

多分类问题的输出层，可以给出多个类别的概率预测；结果区间(0,1)，且总和为1

* ==tanh==    $\tanh(x) = \frac{{e^x - e^{-x}}}{{e^x + e^{-x}}}$

输出范围在(-1,1)之间，是零中心的，可以帮助数据在训练初期更快地收敛；**归一化处理**

cons: 梯度消失



### 混淆矩阵(F1,ROC-AUC)

![image-20230615024624055](C:\Users\NP_123\AppData\Roaming\Typora\typora-user-images\image-20230615024624055.png)

准确率$accuracy=\frac{TP+TN}{TP+TN+FP+FN}$

精确率$precision=\frac{TP}{TP+FP}$  ==预测中的实际P/预测P==

召回率(灵敏度)$sensitivity=recall=\frac{TP}{TP+FN}$  ==预测P/实际P== 模型召回了多少实际为正的样本

特异性$specificity = \frac{TN}{TN+FP}$ ==预测N/实际N== 模型正确排除了多少实际为负的样本

统计学第一类错误(假阳性): FP     统计学第二类错误(假阴性): FN

$F1=2\times \frac{精确率\times 召回率}{精确率+召回率}$ 平衡精确率与召回率

* ROC-AUC

ROC空间将**假阳性率（$1-特异性$ FPR）**定义为 *X* 轴，**真阳性率（召回率 TPR）**定义为 *Y* 轴。

如图所示，越靠近左上的点分类效果越好

<img src="D:\Work\1513-Leetcode\AI\pic\roc_auc.webp" alt="roc_auc" style="zoom:50%;" />

![roc_auc2](D:\Work\1513-Leetcode\AI\pic\roc_auc2.webp)

好处：对类别不平衡数据具有鲁棒性；对阈值选择的依赖性较小；适用于多类别问题的扩展

### 经验误差、泛化误差

经验误差（训练误差）：模型在训练集上的误差称为“经验误差”或者“训练误差”“training error”。 ==做题时的错误率==

泛化误差：模型在新样本集（测试集）上的误差称为“泛化误差”（generalization error）。==考试时的错误率==

$泛化误差=偏差+方差+噪声$

偏差：描述了模型的期望预测（模型的预测结果的期望）与真实结果的偏离程度。

方差：描述了数据的扰动造成的模型性能的变化，即模型在不同数据集上的稳定程度。

**偏差大，说明模型欠拟合；方差大，说明模型过拟合**



## 机器学习算法

### 决策树分类

决策树的生成包括两个过程

​       树的构建

* 首先所有的训练样本都在根结点

* 基于所选的属性循环的划分样本

  树剪枝：识别和删除那些反映噪声或离群点的分支

### 急切学习法 惰性学习法

* 惰性学习（KNN ): 简单存储训练数据 （或只是稍加处理 ) 并且一直等到给定一个检验元组。**直到接收到预测请求时才从数据中学习的策略。即时学习**

优点：可以处理复杂的模式和大规模的训练数据。

缺点：预测可能需要大量的时间和计算资源

* 急切学习 (决策树、神经网络、支持向量机SVM等): 给定训练元组的集合，在收到新的测试数据进行分类之前先构造一个分类器模型。**在学习阶段即构建最终模型的方法。在接收到数据后，它会立即从数据中学习并形成一个泛化的模型，用于后续的预测任务。**

优点：预测速度快，因为目标函数已经在训练阶段就学习好了

缺点：需要大量的时间和计算资源来处理大规模的训练数据

### SVM

* 落在超平面 H 1 或 H 2 即定义边缘的两侧 上的训练元组称为 **支持向量**
* 线性不可分怎么办？转换原始输入数据到一个更高维的空间

### KNN

对于一个需要预测的输入向量x，我们只需要在训练数据集中寻找k个与向量x最近的向量的集合，然后把x的类别预测为这k个样本中类别数最多的那一类。

原理简单、易于理解，可以应用于多分类问题，同时也无需假设数据的分布



​		

### K-means和K-medoids

**K-means**和**K-medoids**都是非常常用的**无监督学习**算法，主要用于聚类分析。它们都通过将数据点分组成K个簇，来发现数据集中的结构或模式。

**相同点**：

1. 都是**分区**的聚类算法，将数据点划分为多个不重叠的簇。
2. 都需要**预先设定簇的数量（K）**。
3. 都采用**迭代**的方式优化簇的划分，以最小化簇内点与中心点之间的距离。
4. 在初始阶段，**都会随机选择K个点作为簇的中心点**。

**不同点**：

1. K-means的中心点（称为质心）是簇内所有点的坐标的平均值，**可以不是实际存在的数据**点；而K-medoids的中心点（称为medoid）**是簇内的一个实际存在的数据点**，**这个点到簇内其他点的平均距离最小**。
2. 因为K-medoids的中心点是实际存在的数据点，所以K-medoids比K-means更能抵抗噪声和异常值的影响。

* **K-means的优缺点**：

优点：

1. 复杂度低
2. 相对可伸缩和高效
3. 不能保证得到全局最优解，通常以局部最优解结束。

缺点：

1. 只有在簇的平均值被定义的情况下才能使用，当涉及有分类属性的数据时无法处理
2. 需要事先给出 k ，簇的数目
3. 对噪声和离群点数据敏感
4. 不适合发现非凸形状的簇，或者大小差别很大的簇

* **K-medoids的优缺点**：

优点：

1. 能够更好地处理噪声和异常值。
2. 由于中心点是实际的数据点，**结果更易于解释**。
3. 可以用任何定义在对象对上的距离或相似度函数。

缺点：

1. **计算复杂度高，尤其在大规模数据集上**，计算成本和时间成本都比K-means高。
2. 同样需要==**预先设定K值**==，但实际应用中K值往往是不知道的。

### BIRCH(未完成)

增量的构造 CF 树 

1. 优点
   线性伸缩性
   支持增量聚类
2. 缺点
   只能处理数值数据，对数据记录的顺序很敏感

### 层次聚类

使用**距离矩阵(二维矩阵，其中包含了数据集中每对样本之间的距离信息)**作为聚类的标准，随着聚类的进行更新距离矩阵。这种方法不需要簇的数目 k 作为输入 但需要一个终止条件

**优点**：

1. **可解释性**：层次聚类的结果通常以树状图（或者说是谱系图）的形式呈现，非常直观，易于理解和解释。
2. ==**不需要预设簇的数量**==：不像K-means或K-medoids等分区方法，层次聚类不需要预先设定簇的数量。用户可以通过切割树状图来选择合适的簇的数量。
3. **提供多层次的聚类**：层次聚类提供了不同层次的聚类结果，可以根据需要选择不同的层次。
4. **可以处理任何形状的簇**：层次聚类不像K-means那样假设簇是凸的或者球状的，它可以处理任何形状的簇。

**缺点**：

1. **计算复杂度高**：层次聚类的计算复杂度较高，$O(n^2)以上$，尤其在大规模数据集上。最常用的算法（比如agglomerative层次聚类）的时间复杂度为O(n^2 log(n))，其中n是数据点的数量。
2. ==**对噪声和异常值敏感**==：层次聚类对噪声和异常值比较敏感，这可能会影响聚类的结果。
3. ==**不能撤销**==：一旦层次聚类的过程完成，就无法再调整。例如，如果在某一步合并了两个簇，之后就无法再将它们分开。
4. **质量的一致性**：层次聚类的质量在不同的数据集和不同的应用中可能会有很大的差异，尤其是在选择不同的距离度量和链接方式时。

### 划分聚类

**划分聚类**：

优点：

1. **算法通常比层次聚类更快**，尤其是在大规模数据集上。
2. K-means等划分聚类方法对大规模数据集和高维数据有较好的扩展性。
3. 对于球形或凸形簇，可以得到较好的结果。

缺点：

1. **必须预先指定聚类的数量**。
2. **划分聚类算法通常对初始簇中心的选择敏感，可能会陷入局部最优**。
3. ==**对噪声和异常值敏感。**==
4. **通常假设簇是凸的或球形的**，对于其他形状的簇可能无法得到好的结果。

在选择层次聚类还是划分聚类时，需要考虑数据集的特性（例如大小、维度和簇的形状等），以及具体的应用需求（例如是否需要层次结构、计算成本的承受能力、对噪声和异常值的容忍度等）。

### 基于密度分类

该类方法把簇看作是数据空间中被低密度区域分割开的高密度对象区域。**基于密度的簇是密度相连的点的集合**

- 能够发现任意形状的簇
- 能处理噪声(==优点，对噪声不敏感==)
- 只需一次扫描
- 需要密度参数作为终结条件

基于密度的划分方法：DB-scan（一个基于高密度连接区域的密度聚类方法）



# Deep-Learning

* Torch的Tensor：多维数组，可以通过GPU进行并行加速计算
* 向量的打包和解包：将一个批次中不同长度的序列（句子）紧凑后放入一个Tensor里。打包前会进行排序，将最长的序列放在前面。之后放入GRU进行处理，处理后再解包回原来的序列

* 最大池化：从一组值中选取最大值，最大池化减少了数据的维度
* Dropout的意义：减少过拟合、防止神经元共适应（使得每个神经元独立）、提高集成模型性能(训练不同的子网络)


